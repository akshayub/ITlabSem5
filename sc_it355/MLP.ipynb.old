{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "classes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the database and also assigning the classes a variable\n",
    "def load(filename):\n",
    "    r = pd.read_csv(filename)\n",
    "    data = np.asarray(r)\n",
    "    np.random.shuffle(data)\n",
    "    class_label = set(data[:,-1])\n",
    "    for i,x in enumerate(list(class_label)):\n",
    "        classes[x] = i\n",
    "        \n",
    "    for row in data:\n",
    "        row[-1] = classes[row[-1]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(data, foldNum):\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    for i,x in enumerate(data):\n",
    "        if i%10 == foldNum:\n",
    "            test.append(x)\n",
    "        else:\n",
    "            train.append(x)\n",
    "\n",
    "    return np.asarray(train), np.asarray(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(lr, train_data, bias_hidden, bias_output, weight_ip_hd, weight_hd_op, iters=500):\n",
    "    for y in range(iters):\n",
    "        error_hd = []\n",
    "        for row in train_data:\n",
    "            # First find the dot product of each row of the weight with the input.\n",
    "            layer_1 = np.array(np.sum(row[:-1] * weight_ip_hd, axis=1) + bias_hidden, dtype=np.float64)\n",
    "            layer_1 = np.apply_along_axis(sigmoid,0,layer_1)\n",
    "            sig_vals = np.copy(layer_1)\n",
    "            # Now for the final layer\n",
    "            val_op = np.dot(layer_1,weight_hd_op) + bias_output\n",
    "            output = sigmoid(val_op)\n",
    "            # Now get the actual output for the row\n",
    "            actual_output = row[-1]\n",
    "            output_error = output * (1 - output) * (actual_output - output)\n",
    "            bias_output = bias_output + (lr * output_error)\n",
    "            # Backprop\n",
    "            error_hd = np.multiply(np.multiply(sig_vals, (1 - sig_vals)), (weight_hd_op * output_error))\n",
    "            weight_hd_op = weight_hd_op + (lr * sig_vals * output_error)\n",
    "            bias_hidden = bias_hidden + (lr * error_hd)\n",
    "            \n",
    "            # Finished with one, another one begins\n",
    "            error_hd = error_hd.reshape(1,len(bias_hidden))\n",
    "            weight_ip_hd = weight_ip_hd + (lr * (error_hd.T * row[:-1]))\n",
    "\n",
    "    return np.asarray(weight_ip_hd), np.asarray(weight_hd_op), np.asarray(bias_hidden), np.asarray(bias_output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing(test_data, bias_hidden, bias_output, weight_ip_hd, weight_hd_op, threshold):\n",
    "    tp = tn = fp = fn = 0.0\n",
    "    for row in test_data:\n",
    "        layer_1 = np.array(np.sum(row[:-1] * weight_ip_hd, axis=1) + bias_hidden, dtype=np.float64)\n",
    "        layer_1 = np.apply_along_axis(sigmoid,0,layer_1)\n",
    "        # Now for the final layer\n",
    "        val_op = np.dot(layer_1,weight_hd_op) + bias_output\n",
    "        output = sigmoid(val_op)\n",
    "        \n",
    "        if output >= threshold:\n",
    "            pred_output = 1\n",
    "        else:\n",
    "            pred_output = 0\n",
    "        \n",
    "        actual_output = row[-1]\n",
    "        \n",
    "        print pred_output, actual_output, output\n",
    "        \n",
    "        if (pred_output == actual_output):\n",
    "            if pred_output == 1.0:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if pred_output == 1.0:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "1 1 0.844738273177\n",
      "1 1 0.890878674239\n",
      "0 0 0.288553339947\n",
      "1 1 0.672346140652\n",
      "1 0 0.883230627713\n",
      "1 1 0.733791478999\n",
      "0 0 0.284276686554\n",
      "1 1 0.890879365533\n",
      "0 0 0.284138971949\n",
      "0 0 0.286945129967\n",
      "0 1 0.286516176942\n",
      "0 0 0.286173165305\n",
      "1 1 0.890878906811\n",
      "1 1 0.890879358097\n",
      "0 0 0.284198114006\n",
      "1 1 0.890879364473\n",
      "0 0 0.295779616514\n",
      "1 1 0.852673868031\n",
      "0.888888888889 0.9 0.9\n",
      "0 1 0.31357146809\n",
      "0 0 0.24868772122\n",
      "0 0 0.242049341304\n",
      "0 0 0.242377137371\n",
      "1 1 0.945561836355\n",
      "1 1 0.945695783032\n",
      "0 1 0.304776743587\n",
      "0 1 0.257753605117\n",
      "1 1 0.945454109106\n",
      "0 0 0.25965832328\n",
      "0 0 0.242168957078\n",
      "1 1 0.945695783089\n",
      "1 1 0.945693540916\n",
      "0 1 0.243929864316\n",
      "0 1 0.244609431498\n",
      "1 1 0.945619226899\n",
      "1 1 0.943986405319\n",
      "1 1 0.945695783101\n",
      "0.722222222222 1.0 0.615384615385\n",
      "0 0 0.238669083071\n",
      "0 0 0.228056983623\n",
      "0 0 0.228619347352\n",
      "1 1 0.960971515725\n",
      "0 0 0.228147884928\n",
      "1 1 0.957713333097\n",
      "0 1 0.228100715935\n",
      "0 1 0.238780227882\n",
      "0 0 0.228057018346\n",
      "0 1 0.228056975689\n",
      "0 1 0.228056984084\n",
      "1 1 0.960970764162\n",
      "0 0 0.247214209817\n",
      "0 0 0.228057840871\n",
      "1 1 0.960971515718\n",
      "1 0 0.956094652029\n",
      "1 0 0.956094652029\n",
      "1 1 0.957394961111\n",
      "0.666666666667 0.714285714286 0.555555555556\n",
      "1 1 0.963699969019\n",
      "0 1 0.273942239799\n",
      "1 1 0.963383649564\n",
      "0 1 0.15201322189\n",
      "1 1 0.963699967535\n",
      "0 0 0.154336684439\n",
      "1 1 0.963699969019\n",
      "1 1 0.963156720588\n",
      "0 1 0.152013465276\n",
      "0 1 0.152013223247\n",
      "1 1 0.963699969019\n",
      "0 0 0.15805171216\n",
      "0 0 0.63672547189\n",
      "1 1 0.963567225038\n",
      "0 1 0.152013955652\n",
      "1 1 0.96367530918\n",
      "0 1 0.152013221221\n",
      "0 0 0.152013221578\n",
      "0.666666666667 1.0 0.571428571429\n",
      "0 1 0.141316137123\n",
      "0 0 0.141291799048\n",
      "0 0 0.141315952922\n",
      "0 0 0.141297324799\n",
      "0 1 0.144659762792\n",
      "0 1 0.141293696923\n",
      "1 1 0.968587660282\n",
      "1 1 0.968274448283\n",
      "0 1 0.141353401138\n",
      "0 1 0.141291896848\n",
      "0 0 0.141365671895\n",
      "0 0 0.141516501919\n",
      "0 0 0.141291821825\n",
      "0 0 0.14136751436\n",
      "0 1 0.141291923577\n",
      "1 1 0.968587660282\n",
      "0 1 0.141291862675\n",
      "1 1 0.968587660282\n",
      "0.611111111111 1.0 0.363636363636\n",
      "1 1 0.975855553572\n",
      "1 1 0.975839543397\n",
      "0 1 0.257529538468\n",
      "1 1 0.971829677415\n",
      "0 0 0.202666008708\n",
      "0 0 0.202482484458\n",
      "0 0 0.202795501854\n",
      "0 1 0.207174600322\n",
      "1 0 0.968882081923\n",
      "0 1 0.203856236936\n",
      "1 1 0.97585555366\n",
      "0 0 0.202539323\n",
      "1 1 0.974369535251\n",
      "0 0 0.228888711438\n",
      "0 0 0.202477476062\n",
      "0 0 0.202480914381\n",
      "0 0 0.202795501854\n",
      "1 1 0.975395509626\n",
      "0.777777777778 0.857142857143 0.666666666667\n",
      "1 1 0.93452433193\n",
      "1 1 0.979619772684\n",
      "0 0 0.174659985258\n",
      "0 0 0.175352543262\n",
      "0 0 0.1745190319\n",
      "1 1 0.979627272259\n",
      "1 1 0.979627259677\n",
      "0 0 0.174280137269\n",
      "0 1 0.175913436295\n",
      "0 0 0.174568221051\n",
      "1 1 0.979627272273\n",
      "1 1 0.979627272266\n",
      "1 1 0.977831826255\n",
      "1 1 0.927572337398\n",
      "1 1 0.979627214732\n",
      "0 1 0.174263508006\n",
      "0 0 0.174263515792\n",
      "1 1 0.979613171571\n",
      "0.888888888889 1.0 0.833333333333\n",
      "1 1 0.982127551534\n",
      "1 1 0.982263762048\n",
      "0 0 0.188471300006\n",
      "0 0 0.189169632301\n",
      "1 1 0.982270983628\n",
      "1 1 0.982266500585\n",
      "1 1 0.982170721177\n",
      "1 1 0.982259447458\n",
      "1 1 0.982271026942\n",
      "0 0 0.188400034069\n",
      "1 1 0.982270670587\n",
      "1 1 0.982270532648\n",
      "1 1 0.982271027537\n",
      "1 1 0.981925939847\n",
      "0 0 0.188376656234\n",
      "0 0 0.195083138296\n",
      "1 1 0.98225416758\n",
      "0 0 0.540142625779\n",
      "1.0 1.0 1.0\n",
      "1 1 0.984338951502\n",
      "1 1 0.713868970161\n",
      "0 0 0.176768845308\n",
      "0 0 0.176364490522\n",
      "1 1 0.983206256969\n",
      "1 1 0.842888259622\n",
      "1 1 0.98433877559\n",
      "1 1 0.984338951508\n",
      "0 0 0.176066766565\n",
      "1 1 0.984338951508\n",
      "0 1 0.17606679592\n",
      "0 0 0.176298263946\n",
      "1 1 0.984338951469\n",
      "1 1 0.984338951508\n",
      "0 0 0.176066766769\n",
      "1 1 0.937284014984\n",
      "0 0 0.176066832157\n",
      "1 1 0.984338951508\n",
      "0.944444444444 1.0 0.916666666667\n",
      "0 0 0.196174061222\n",
      "0 0 0.210809910473\n",
      "1 1 0.854022393232\n",
      "1 1 0.985969198269\n",
      "1 1 0.74116717976\n",
      "0 0 0.196246978671\n",
      "0 0 0.196896902293\n",
      "1 1 0.983901277966\n",
      "0 0 0.196166763557\n",
      "1 1 0.985980013081\n",
      "0 0 0.197325945261\n",
      "1 1 0.985980507145\n",
      "1 1 0.985980476638\n",
      "1 1 0.985980507145\n",
      "0 0 0.196651034494\n",
      "1 1 0.985980491761\n",
      "0 0 0.196165107998\n",
      "0 0 0.196171382425\n",
      "1.0 1.0 1.0\n",
      "Accuracy = 0.816666666667, Precision = 0.947142857143, Recall = 0.742267177267\n"
     ]
    }
   ],
   "source": [
    "data = load('IRIS.csv')\n",
    "# Variables\n",
    "print classes\n",
    "features = len(data[0]) - 1\n",
    "data_size = len(data)\n",
    "learning_rate = 0.1\n",
    "hidden_layer_nodes = 5\n",
    "threshold = 0.65\n",
    "\n",
    "bias_at_hidden = 5\n",
    "bias_at_output = 1\n",
    "bias_hidden = np.array([bias_at_hidden] * hidden_layer_nodes)\n",
    "\n",
    "wt_ip_hd = [[1.0/(features * hidden_layer_nodes + bias_at_hidden)] * features] * hidden_layer_nodes\n",
    "wt_ip_hd = np.asarray(wt_ip_hd)\n",
    "\n",
    "wt_hd_op = [1.0/(hidden_layer_nodes + bias_at_hidden)] * hidden_layer_nodes\n",
    "wt_hd_op = np.asarray(wt_hd_op)\n",
    "\n",
    "#Split the training and testing\n",
    "accuracy = precision = recall = 0\n",
    "for i in range(10): #1 folds\n",
    "    train, test = split(data, i)\n",
    "    \n",
    "    wt_ip_hd, wt_hd_op, bias_hidden, bias_at_output = training(\n",
    "        learning_rate,\n",
    "        train,\n",
    "        bias_hidden,\n",
    "        bias_at_output,\n",
    "        wt_ip_hd,\n",
    "        wt_hd_op,\n",
    "        iters=500\n",
    "    )\n",
    "    \n",
    "    tp, tn, fp, fn = testing(test, bias_hidden, bias_at_output, wt_ip_hd, wt_hd_op, threshold)\n",
    "    try:\n",
    "        pre = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        pre = 0\n",
    "    try:\n",
    "        rec = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        rec = 0\n",
    "        \n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print acc, pre, rec\n",
    "    \n",
    "    accuracy += acc\n",
    "    precision += pre\n",
    "    recall += rec\n",
    "    \n",
    "accuracy /= 10\n",
    "precision /= 10\n",
    "recall /= 10\n",
    "\n",
    "print \"Accuracy = {}, Precision = {}, Recall = {}\".format(accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

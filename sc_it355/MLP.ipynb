{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our sigmoid function, tanh is a little nicer than the standard 1/(1+e^-x)\n",
    "def sigmoid(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(y):\n",
    "    return 1.0 - y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNet:\n",
    "    bias_output = 1.0\n",
    "    bias_hidden = 5.0\n",
    "    # bias = np.random.rand()\n",
    "    lr = 0.1\n",
    "    times_with_different_lrs = 1\n",
    "    training_iterations = 500\n",
    "    number_of_folds = 10\n",
    "    hidden_layers = 1\n",
    "    hidden_nodes = [5]\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data = np.asarray(pd.read_csv(filename))\n",
    "        self.classes = set()\n",
    "        for _ in self.data:\n",
    "            self.classes.add(_[-1])\n",
    "        self.classes = dict(zip(self.classes, range(len(self.classes))))\n",
    "        np.random.shuffle(self.data)\n",
    "        self.test_data_size = len(self.data)/self.number_of_folds\n",
    "        \n",
    "        self.features = len(self.data[0])-1\n",
    "        self.i = self.features + 1 #bias\n",
    "        #self.wi = np.asarray([np.random.rand(self.hidden_nodes) for each in xrange(self.features)])\n",
    "        #self.wh = np.random.rand(self.hidden_nodes)\n",
    "        \n",
    "        # bias weight is always 1. only bias is added\n",
    "        self.wi = np.asarray(\n",
    "        [\n",
    "            np.asarray([1.0/(self.features*self.hidden_nodes[0] + self.bias_hidden)]*self.hidden_nodes[0]) for _ in xrange(self.features)\n",
    "        ])\n",
    "        # 0th index is for first hidden node\n",
    "        self.wh = np.asarray(\n",
    "        [\n",
    "            np.asarray([1.0/(self.hidden_nodes[0] + self.bias_output)]*self.hidden_nodes[0])\n",
    "        ])\n",
    "        \n",
    "    def splitDataset(self, fold):\n",
    "        testStart = fold*self.test_data_size\n",
    "        testEnd = (fold+1)*self.test_data_size\n",
    "\n",
    "        test = data[testStart:testEnd]\n",
    "        train = np.concatenate((data[:testStart], data[testEnd:]), axis = 0)\n",
    "\n",
    "        return np.asarray(train), np.asarray(test)\n",
    "        \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "        \n",
    "x = neuralNet('datasets/IRIS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, lr, weights, bias, classes, shouldBiasChange, iters=1):\n",
    "    for ___ in xrange(iters):\n",
    "        for each in data:\n",
    "            predicted_val = np.dot(each[:-1], weights) + bias\n",
    "#             print predicted_val, each[:-1], weights\n",
    "            predicted_val = int(np.clip(np.sign(predicted_val),0,1))\n",
    "            actual_val = classes[each[-1]]\n",
    "#             print predicted_val, '\\t',actual_val\n",
    "            error = predicted_val - actual_val\n",
    "            delW = lr * error * each[:-1]\n",
    "#             print error, delW\n",
    "            weights = weights - delW\n",
    "            if shouldBiasChange:\n",
    "                bias = bias - lr*error\n",
    "    return weights, bias\n",
    "\n",
    "def training(lr, train_data, bias_hidden, bias_output, weight_ip_hd, weight_hd_op, iters=500):\n",
    "    for y in range(iters):\n",
    "        error_hd = []\n",
    "        for row in train_data:\n",
    "            # First find the dot product of each row of the weight with the input.\n",
    "            layer_1 = np.array(np.sum(row[:-1] * weight_ip_hd, axis=1) + bias_hidden, dtype=np.float64)\n",
    "            layer_1 = np.apply_along_axis(sigmoid,0,layer_1)\n",
    "            sig_vals = np.copy(layer_1)\n",
    "            # Now for the final layer\n",
    "            val_op = np.dot(layer_1,weight_hd_op) + bias_output\n",
    "            output = sigmoid(val_op)\n",
    "            # Now get the actual output for the row\n",
    "            actual_output = row[-1]\n",
    "            output_error = output * (1 - output) * (actual_output - output)\n",
    "            bias_output = bias_output + (lr * output_error)\n",
    "            # Backprop\n",
    "            error_hd = np.multiply(np.multiply(sig_vals, (1 - sig_vals)), (weight_hd_op * output_error))\n",
    "            weight_hd_op = weight_hd_op + (lr * sig_vals * output_error)\n",
    "            bias_hidden = bias_hidden + (lr * error_hd)\n",
    "            \n",
    "            # Finished with one, another one begins\n",
    "            error_hd = error_hd.reshape(1,len(bias_hidden))\n",
    "            weight_ip_hd = weight_ip_hd + (lr * (error_hd.T * row[:-1]))\n",
    "\n",
    "    return np.asarray(weight_ip_hd), np.asarray(weight_hd_op), np.asarray(bias_hidden), np.asarray(bias_output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(data, weights, bias, classes):\n",
    "    tp = tn = fp = fn = 0\n",
    "    \n",
    "    for each in data:\n",
    "        predicted_val = np.dot(each[:-1], weights) + bias\n",
    "        predicted_val = int(np.clip(np.sign(predicted_val),0,1))\n",
    "\n",
    "        if (predicted_val == classes[each[-1]]):\n",
    "            if predicted_val == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if predicted_val == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    \n",
    "    accuracy = (tp + tn)*100/(tp + tn + fp + fn)\n",
    "    try:\n",
    "        precision = (tp)*100/(tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = (tp)*100/(tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "    return (accuracy, precision, recall)\n",
    "\n",
    "def testing(test_data, bias_hidden, bias_output, weight_ip_hd, weight_hd_op, threshold):\n",
    "    tp = tn = fp = fn = 0.0\n",
    "    for row in test_data:\n",
    "        layer_1 = np.array(np.sum(row[:-1] * weight_ip_hd, axis=1) + bias_hidden, dtype=np.float64)\n",
    "        layer_1 = np.apply_along_axis(sigmoid,0,layer_1)\n",
    "        # Now for the final layer\n",
    "        val_op = np.dot(layer_1,weight_hd_op) + bias_output\n",
    "        output = sigmoid(val_op)\n",
    "        \n",
    "        if output >= threshold:\n",
    "            pred_output = 1\n",
    "        else:\n",
    "            pred_output = 0\n",
    "        \n",
    "        actual_output = row[-1]\n",
    "        \n",
    "        print pred_output, actual_output, output\n",
    "        \n",
    "        if (pred_output == actual_output):\n",
    "            if pred_output == 1.0:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if pred_output == 1.0:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parameters\n",
    "\n",
    "filename = 'datasets/IRIS.csv'\n",
    "# filename = 'datasets/SPECTF_New.csv'\n",
    "bias = 1.0\n",
    "shouldBiasChange = False\n",
    "learning_rate = 0.1\n",
    "times_with_different_lrs = 1\n",
    "training_iterations = 500\n",
    "number_of_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, classes = load_data(filename)\n",
    "lr = learning_rate\n",
    "test_data_size = len(data)/number_of_folds\n",
    "\n",
    "features = data.shape[1] - 1\n",
    "# weights = np.random.rand(features)\n",
    "weights = np.asarray([1.0/(features+1)]*features)\n",
    "\n",
    "maxAccuracy = -float('inf')\n",
    "maxAccuracyLr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(times_with_different_lrs):\n",
    "    totalAccuracy = totalPrecision = totalRecall = 0\n",
    "    \n",
    "    print 'For learning rate %.1lf\\n' %lr\n",
    "    weights = np.asarray([1.0/(features+1)]*features)\n",
    "    \n",
    "    for eachFold in range(number_of_folds):\n",
    "        trainSet, testSet = splitDataset(data, eachFold, test_data_size)\n",
    "        weights, bias = train(trainSet, lr, weights, bias, classes, shouldBiasChange, training_iterations)\n",
    "        accuracy, precision, recall = test(testSet, weights, bias, classes)\n",
    "        \n",
    "        print 'Fold %d\\tAccuracy : %lf\\tPrecision : %lf\\tRecall : %lf' %(eachFold+1, accuracy, precision, recall)\n",
    "\n",
    "        totalAccuracy += accuracy\n",
    "        totalPrecision += precision\n",
    "        totalRecall += recall\n",
    "    \n",
    "    totalAccuracy /= float(number_of_folds)\n",
    "    totalPrecision /= float(number_of_folds)\n",
    "    totalRecall /= float(number_of_folds)\n",
    "    \n",
    "    print \"\"\"\n",
    "Accuracy : %lf\\tPrecision : %lf\\tRecall : %lf\n",
    "    \"\"\" %(totalAccuracy, totalPrecision, totalRecall)\n",
    "    \n",
    "    if totalAccuracy > maxAccuracy:\n",
    "        maxAccuracy = totalAccuracy\n",
    "        maxAccuracyLr = lr\n",
    "#     lr += 0.1\n",
    "    print '='*100\n",
    "print 'Max Accuracy is %lf for learning rate %0.1lf' %(maxAccuracy, maxAccuracyLr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "1 1 0.844738273177\n",
      "1 1 0.890878674239\n",
      "0 0 0.288553339947\n",
      "1 1 0.672346140652\n",
      "1 0 0.883230627713\n",
      "1 1 0.733791478999\n",
      "0 0 0.284276686554\n",
      "1 1 0.890879365533\n",
      "0 0 0.284138971949\n",
      "0 0 0.286945129967\n",
      "0 1 0.286516176942\n",
      "0 0 0.286173165305\n",
      "1 1 0.890878906811\n",
      "1 1 0.890879358097\n",
      "0 0 0.284198114006\n",
      "1 1 0.890879364473\n",
      "0 0 0.295779616514\n",
      "1 1 0.852673868031\n",
      "0.888888888889 0.9 0.9\n",
      "0 1 0.31357146809\n",
      "0 0 0.24868772122\n",
      "0 0 0.242049341304\n",
      "0 0 0.242377137371\n",
      "1 1 0.945561836355\n",
      "1 1 0.945695783032\n",
      "0 1 0.304776743587\n",
      "0 1 0.257753605117\n",
      "1 1 0.945454109106\n",
      "0 0 0.25965832328\n",
      "0 0 0.242168957078\n",
      "1 1 0.945695783089\n",
      "1 1 0.945693540916\n",
      "0 1 0.243929864316\n",
      "0 1 0.244609431498\n",
      "1 1 0.945619226899\n",
      "1 1 0.943986405319\n",
      "1 1 0.945695783101\n",
      "0.722222222222 1.0 0.615384615385\n",
      "0 0 0.238669083071\n",
      "0 0 0.228056983623\n",
      "0 0 0.228619347352\n",
      "1 1 0.960971515725\n",
      "0 0 0.228147884928\n",
      "1 1 0.957713333097\n",
      "0 1 0.228100715935\n",
      "0 1 0.238780227882\n",
      "0 0 0.228057018346\n",
      "0 1 0.228056975689\n",
      "0 1 0.228056984084\n",
      "1 1 0.960970764162\n",
      "0 0 0.247214209817\n",
      "0 0 0.228057840871\n",
      "1 1 0.960971515718\n",
      "1 0 0.956094652029\n",
      "1 0 0.956094652029\n",
      "1 1 0.957394961111\n",
      "0.666666666667 0.714285714286 0.555555555556\n",
      "1 1 0.963699969019\n",
      "0 1 0.273942239799\n",
      "1 1 0.963383649564\n",
      "0 1 0.15201322189\n",
      "1 1 0.963699967535\n",
      "0 0 0.154336684439\n",
      "1 1 0.963699969019\n",
      "1 1 0.963156720588\n",
      "0 1 0.152013465276\n",
      "0 1 0.152013223247\n",
      "1 1 0.963699969019\n",
      "0 0 0.15805171216\n",
      "0 0 0.63672547189\n",
      "1 1 0.963567225038\n",
      "0 1 0.152013955652\n",
      "1 1 0.96367530918\n",
      "0 1 0.152013221221\n",
      "0 0 0.152013221578\n",
      "0.666666666667 1.0 0.571428571429\n",
      "0 1 0.141316137123\n",
      "0 0 0.141291799048\n",
      "0 0 0.141315952922\n",
      "0 0 0.141297324799\n",
      "0 1 0.144659762792\n",
      "0 1 0.141293696923\n",
      "1 1 0.968587660282\n",
      "1 1 0.968274448283\n",
      "0 1 0.141353401138\n",
      "0 1 0.141291896848\n",
      "0 0 0.141365671895\n",
      "0 0 0.141516501919\n",
      "0 0 0.141291821825\n",
      "0 0 0.14136751436\n",
      "0 1 0.141291923577\n",
      "1 1 0.968587660282\n",
      "0 1 0.141291862675\n",
      "1 1 0.968587660282\n",
      "0.611111111111 1.0 0.363636363636\n",
      "1 1 0.975855553572\n",
      "1 1 0.975839543397\n",
      "0 1 0.257529538468\n",
      "1 1 0.971829677415\n",
      "0 0 0.202666008708\n",
      "0 0 0.202482484458\n",
      "0 0 0.202795501854\n",
      "0 1 0.207174600322\n",
      "1 0 0.968882081923\n",
      "0 1 0.203856236936\n",
      "1 1 0.97585555366\n",
      "0 0 0.202539323\n",
      "1 1 0.974369535251\n",
      "0 0 0.228888711438\n",
      "0 0 0.202477476062\n",
      "0 0 0.202480914381\n",
      "0 0 0.202795501854\n",
      "1 1 0.975395509626\n",
      "0.777777777778 0.857142857143 0.666666666667\n",
      "1 1 0.93452433193\n",
      "1 1 0.979619772684\n",
      "0 0 0.174659985258\n",
      "0 0 0.175352543262\n",
      "0 0 0.1745190319\n",
      "1 1 0.979627272259\n",
      "1 1 0.979627259677\n",
      "0 0 0.174280137269\n",
      "0 1 0.175913436295\n",
      "0 0 0.174568221051\n",
      "1 1 0.979627272273\n",
      "1 1 0.979627272266\n",
      "1 1 0.977831826255\n",
      "1 1 0.927572337398\n",
      "1 1 0.979627214732\n",
      "0 1 0.174263508006\n",
      "0 0 0.174263515792\n",
      "1 1 0.979613171571\n",
      "0.888888888889 1.0 0.833333333333\n",
      "1 1 0.982127551534\n",
      "1 1 0.982263762048\n",
      "0 0 0.188471300006\n",
      "0 0 0.189169632301\n",
      "1 1 0.982270983628\n",
      "1 1 0.982266500585\n",
      "1 1 0.982170721177\n",
      "1 1 0.982259447458\n",
      "1 1 0.982271026942\n",
      "0 0 0.188400034069\n",
      "1 1 0.982270670587\n",
      "1 1 0.982270532648\n",
      "1 1 0.982271027537\n",
      "1 1 0.981925939847\n",
      "0 0 0.188376656234\n",
      "0 0 0.195083138296\n",
      "1 1 0.98225416758\n",
      "0 0 0.540142625779\n",
      "1.0 1.0 1.0\n",
      "1 1 0.984338951502\n",
      "1 1 0.713868970161\n",
      "0 0 0.176768845308\n",
      "0 0 0.176364490522\n",
      "1 1 0.983206256969\n",
      "1 1 0.842888259622\n",
      "1 1 0.98433877559\n",
      "1 1 0.984338951508\n",
      "0 0 0.176066766565\n",
      "1 1 0.984338951508\n",
      "0 1 0.17606679592\n",
      "0 0 0.176298263946\n",
      "1 1 0.984338951469\n",
      "1 1 0.984338951508\n",
      "0 0 0.176066766769\n",
      "1 1 0.937284014984\n",
      "0 0 0.176066832157\n",
      "1 1 0.984338951508\n",
      "0.944444444444 1.0 0.916666666667\n",
      "0 0 0.196174061222\n",
      "0 0 0.210809910473\n",
      "1 1 0.854022393232\n",
      "1 1 0.985969198269\n",
      "1 1 0.74116717976\n",
      "0 0 0.196246978671\n",
      "0 0 0.196896902293\n",
      "1 1 0.983901277966\n",
      "0 0 0.196166763557\n",
      "1 1 0.985980013081\n",
      "0 0 0.197325945261\n",
      "1 1 0.985980507145\n",
      "1 1 0.985980476638\n",
      "1 1 0.985980507145\n",
      "0 0 0.196651034494\n",
      "1 1 0.985980491761\n",
      "0 0 0.196165107998\n",
      "0 0 0.196171382425\n",
      "1.0 1.0 1.0\n",
      "Accuracy = 0.816666666667, Precision = 0.947142857143, Recall = 0.742267177267\n"
     ]
    }
   ],
   "source": [
    "data = load('IRIS.csv')\n",
    "# Variables\n",
    "print classes\n",
    "features = len(data[0]) - 1\n",
    "data_size = len(data)\n",
    "learning_rate = 0.1\n",
    "hidden_layer_nodes = 5\n",
    "threshold = 0.65\n",
    "\n",
    "bias_at_hidden = 5\n",
    "bias_at_output = 1\n",
    "bias_hidden = np.array([bias_at_hidden] * hidden_layer_nodes)\n",
    "\n",
    "wt_ip_hd = [[1.0/(features * hidden_layer_nodes + bias_at_hidden)] * features] * hidden_layer_nodes\n",
    "wt_ip_hd = np.asarray(wt_ip_hd)\n",
    "\n",
    "wt_hd_op = [1.0/(hidden_layer_nodes + bias_at_hidden)] * hidden_layer_nodes\n",
    "wt_hd_op = np.asarray(wt_hd_op)\n",
    "\n",
    "#Split the training and testing\n",
    "accuracy = precision = recall = 0\n",
    "for i in range(10): #1 folds\n",
    "    train, test = split(data, i)\n",
    "    \n",
    "    wt_ip_hd, wt_hd_op, bias_hidden, bias_at_output = training(\n",
    "        learning_rate,\n",
    "        train,\n",
    "        bias_hidden,\n",
    "        bias_at_output,\n",
    "        wt_ip_hd,\n",
    "        wt_hd_op,\n",
    "        iters=500\n",
    "    )\n",
    "    \n",
    "    tp, tn, fp, fn = testing(test, bias_hidden, bias_at_output, wt_ip_hd, wt_hd_op, threshold)\n",
    "    try:\n",
    "        pre = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        pre = 0\n",
    "    try:\n",
    "        rec = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        rec = 0\n",
    "        \n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print acc, pre, rec\n",
    "    \n",
    "    accuracy += acc\n",
    "    precision += pre\n",
    "    recall += rec\n",
    "    \n",
    "accuracy /= 10\n",
    "precision /= 10\n",
    "recall /= 10\n",
    "\n",
    "print \"Accuracy = {}, Precision = {}, Recall = {}\".format(accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
